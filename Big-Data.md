
[Big data](https://en.m.wikipedia.org/wiki/Big_data) is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (columns) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

[Big data](https://www.guru99.com/bigdata-tutorials.html) was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Therefore, big data often includes data with sizes that exceed the capacity of traditional software to process within an acceptable time and value.

Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. 

The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s.

Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 [zettabytes](https://en.m.wikipedia.org/wiki/Byte#Multiple-byte_units) to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data.

[Big data ethics](https://en.m.wikipedia.org/wiki/Big_data_ethics) also known as simply data ethics refers to systemizing, defending, and recommending concepts of right and wrong conduct in relation to data, in particular personal data.

## Hadoop 

[Apache Hadoop](https://en.m.wikipedia.org/wiki/Apache_Hadoop) is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Hadoop was originally designed for computer clusters built from commodity hardware, which is still the common use. 

 

## Spark

[Apache Spark](https://en.m.wikipedia.org/wiki/Apache_Spark) is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation.

Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.

The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.

Spark and its RDDs were developed in 2012 in response to [limitations](https://www.google.com/amp/s/data-flair.training/blogs/13-limitations-of-hadoop/amp/) in the [MapReduce](https://en.m.wikipedia.org/wiki/MapReduce) cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. 

Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory
